import os
import asyncio
import aiohttp
import redis
import hashlib
import pickle
import uuid
import time
import json
import psutil
from pathlib import Path
from typing import List, Dict
from datetime import datetime
from io import BytesIO

from openai import OpenAI
from fastapi import FastAPI, UploadFile, File, HTTPException, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse
import uvicorn
from neo4j import GraphDatabase
from contextlib import asynccontextmanager
from docx import Document
import pdfplumber

from dotenv import load_dotenv
from SPARQLWrapper import SPARQLWrapper, JSON as SPARQL_JSON
import requests
from cachetools import TTLCache

load_dotenv()

# ===========================================
# å…¨å±€å˜é‡å’Œé…ç½®
# ===========================================

processing_status = {}
cache_manager = None

# åˆå§‹åŒ– Redis ç¼“å­˜
try:
    cache_manager = redis.Redis(
        host=os.getenv('REDIS_HOST', 'redis'),
        port=int(os.getenv('REDIS_PORT', 6379)),
        db=0,
        decode_responses=False,
        socket_connect_timeout=5,
        socket_timeout=5
    )
    cache_manager.ping()
    print("âœ… Redisç¼“å­˜å·²è¿æ¥")
except Exception as e:
    cache_manager = None
    print(f"âš ï¸ Redisæœªè¿æ¥ï¼Œç¼“å­˜åŠŸèƒ½ç¦ç”¨: {e}")


# ===========================================
# ç¼“å­˜å·¥å…·å‡½æ•°
# ===========================================

def get_cache_key(prefix: str, content: str) -> str:
    """ç”Ÿæˆç¼“å­˜é”®"""
    content_hash = hashlib.md5(content.encode()).hexdigest()
    return f"{prefix}:{content_hash}"


async def get_cached_result(cache_key: str):
    """è·å–ç¼“å­˜ç»“æœ"""
    if not cache_manager:
        return None
    try:
        cached = cache_manager.get(cache_key)
        if cached:
            return pickle.loads(cached)
    except Exception as e:
        print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
    return None


async def set_cached_result(cache_key: str, result, expire_seconds: int = 3600):
    """è®¾ç½®ç¼“å­˜ç»“æœ"""
    if not cache_manager:
        return
    try:
        cache_manager.setex(cache_key, expire_seconds, pickle.dumps(result))
    except Exception as e:
        print(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")


# ===========================================
# ç»“æœä¿å­˜å‡½æ•°
# ===========================================

RESULTS_DIR = Path("/app/results")
RESULTS_DIR.mkdir(exist_ok=True)


def save_analysis_result(task_id: str, file_name: str, result_data: dict, processing_time: float):
    """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
    try:
        result_file = RESULTS_DIR / f"{task_id}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                "task_id": task_id,
                "file_name": file_name,
                "processed_at": datetime.now().isoformat(),
                "processing_time": processing_time,
                "result": result_data
            }, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜: {result_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")


# ===========================================
# ç»Ÿä¸€çš„ LLM è°ƒç”¨å‡½æ•°
# ===========================================

async def call_deepseek_api(prompt: str, task_type: str = "general", temperature: float = 0.1) -> Dict:
    """
    ç»Ÿä¸€çš„ DeepSeek API è°ƒç”¨ï¼Œå¸¦ç¼“å­˜
    """
    cache_key = get_cache_key(task_type, prompt)

    # å°è¯•ä»ç¼“å­˜è·å–
    cached_result = await get_cached_result(cache_key)
    if cached_result:
        print(f"ğŸ¯ ä½¿ç”¨ {task_type} ç¼“å­˜ç»“æœ")
        return cached_result

    # API è°ƒç”¨
    connector = aiohttp.TCPConnector(
        limit=10,
        ttl_dns_cache=300,
        use_dns_cache=True
    )
    timeout = aiohttp.ClientTimeout(total=60, connect=10)

    async with aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                "Authorization": f"Bearer {os.getenv('DEEPSEEK_API_KEY')}",
                "Content-Type": "application/json"
            }
    ) as session:
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "system", "content": f"You are a {task_type} expert. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            "temperature": temperature
        }

        try:
            async with session.post("https://api.deepseek.com/v1/chat/completions", json=data) as response:
                if response.status == 200:
                    result_data = await response.json()
                    content = result_data["choices"][0]["message"]["content"].strip()

                    # è§£æ JSON
                    if "```json" in content:
                        content = content.split("```json")[1].split("```")[0].strip()
                    elif "```" in content:
                        content = content.split("```")[1].split("```")[0].strip()

                    result = json.loads(content) if content else {}

                    # ç¼“å­˜ç»“æœ
                    await set_cached_result(cache_key, result)
                    return result
                else:
                    error_text = await response.text()
                    raise Exception(f"APIé”™è¯¯ {response.status}: {error_text}")

        except Exception as e:
            print(f"{task_type} APIè°ƒç”¨é”™è¯¯: {e}")
            return {"error": str(e)}


# ===========================================
# å¤–éƒ¨çŸ¥è¯†åº“ç±»
# ===========================================

class ExternalKnowledge:
    def __init__(self, cache_ttl=3600):
        self.sparql = SPARQLWrapper("https://query.wikidata.org/sparql")
        self.cache = TTLCache(maxsize=1000, ttl=cache_ttl)
        self.api_url = "https://www.wikidata.org/w/api.php"

    def query_wikidata_entity(self, entity_name, lang='zh'):
        cache_key = f"entity_{entity_name}_{lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        query = """
        SELECT ?item ?itemLabel ?itemDescription ?location ?instanceOf
        WHERE {
          ?item rdfs:label "%s"@%s .
          OPTIONAL { ?item wdt:P625 ?location . }
          OPTIONAL { ?item wdt:P31 ?instanceOf . }
          SERVICE wikibase:label { bd:serviceParam wikibase:language "%s,[en]" . }
        }
        LIMIT 1
        """ % (entity_name, lang, lang)
        self.sparql.setQuery(query)
        self.sparql.setReturnFormat(SPARQL_JSON)
        try:
            results = self.sparql.query().convert()
            if results['results']['bindings']:
                binding = results['results']['bindings'][0]
                result = {
                    'qid': binding['item']['value'].split('/')[-1],
                    'label': binding['itemLabel']['value'],
                    'description': binding.get('itemDescription', {}).get('value', ''),
                    'location': binding.get('location', {}).get('value', ''),
                    'instance_of': binding.get('instanceOf', {}).get('value', '').split('/')[-1]
                }
                self.cache[cache_key] = result
                return result
        except Exception as e:
            print(f"WikidataæŸ¥è¯¢é”™è¯¯ {entity_name}: {str(e)}")
        return None

    def get_candidates(self, entity_name, lang='zh'):
        cache_key = f"candidates_{entity_name}_{lang}"
        if cache_key in self.cache:
            return self.cache[cache_key]

        params = {
            'action': 'wbsearchentities',
            'search': entity_name,
            'language': lang,
            'format': 'json',
            'limit': 5
        }
        try:
            response = requests.get(self.api_url, params=params).json()
            candidates = [item['id'] for item in response.get('search', [])]
            self.cache[cache_key] = candidates
            return candidates
        except Exception as e:
            print(f"Wikidataæœç´¢é”™è¯¯ {entity_name}: {str(e)}")
            return [entity_name]

    def compute_similarity(self, candidate_qid, context, lang='zh'):
        cache_key = f"entity_{candidate_qid}_{lang}"
        if cache_key in self.cache:
            candidate_info = self.cache[cache_key]
        else:
            query = """
            SELECT ?itemLabel ?itemDescription
            WHERE {
              wd:%s rdfs:label ?itemLabel .
              OPTIONAL { wd:%s schema:description ?itemDescription . }
              SERVICE wikibase:label { bd:serviceParam wikibase:language "%s,[en]" . }
            }
            LIMIT 1
            """ % (candidate_qid, candidate_qid, lang)
            self.sparql.setQuery(query)
            self.sparql.setReturnFormat(SPARQL_JSON)
            try:
                results = self.sparql.query().convert()
                if results['results']['bindings']:
                    binding = results['results']['bindings'][0]
                    candidate_info = {
                        'qid': candidate_qid,
                        'label': binding['itemLabel']['value'],
                        'description': binding.get('itemDescription', {}).get('value', '')
                    }
                    self.cache[cache_key] = candidate_info
                else:
                    return 0.0
            except Exception as e:
                print(f"ç›¸ä¼¼åº¦æŸ¥è¯¢é”™è¯¯ {candidate_qid}: {str(e)}")
                return 0.0

        description = candidate_info.get('description', '')
        context_words = set(context.lower().split())
        desc_words = set(description.lower().split())
        common_words = len(context_words.intersection(desc_words))
        return common_words / max(len(desc_words), 1)


# ===========================================
# å®ä½“æ¶ˆæ­§ç±»
# ===========================================

class EntityDisambiguation:
    def __init__(self):
        self.knowledge = ExternalKnowledge()

    def disambiguate(self, entity, context):
        try:
            candidates = self.knowledge.get_candidates(entity, 'zh')
            if len(candidates) == 1:
                return candidates[0]
            scores = [(c, self.knowledge.compute_similarity(c, context, 'zh')) for c in candidates]
            best_candidate = max(scores, key=lambda x: x[1], default=(entity, 0))[0]
            return best_candidate if best_candidate else entity
        except Exception as e:
            print(f"æ¶ˆæ­§é”™è¯¯ {entity}: {str(e)}")
            return entity


# ===========================================
# å¤šæ™ºèƒ½ä½“è°ƒåº¦å™¨
# ===========================================

class MultiAgentScheduler:
    def __init__(self):
        self.agent_pool = {}

    def register_agent(self, agent):
        self.agent_pool[agent.name] = agent

    def schedule_task(self, task):
        try:
            task_type = task.get('type')
            agent = self.agent_pool.get(task_type)
            if not agent:
                raise ValueError(f"No agent for {task_type}")
            return agent.execute(task)
        except Exception as e:
            print(f"è°ƒåº¦é”™è¯¯ {task.get('type')}: {str(e)}")
            return {"result": [], "error": f"Scheduling failed: {str(e)}"}


# ===========================================
# æ™ºèƒ½ä½“åŸºç±»
# ===========================================

# å…¨å±€ OpenAI å®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_deepseek_client = None


def get_deepseek_client():
    """è·å–å…¨å±€ DeepSeek å®¢æˆ·ç«¯"""
    global _deepseek_client
    if _deepseek_client is None:
        _deepseek_client = OpenAI(
            api_key=os.getenv("DEEPSEEK_API_KEY"),
            base_url="https://api.deepseek.com/v1"
        )
    return _deepseek_client


class CustomAgent:
    def __init__(self, name, task_type):
        self.name = name
        self.task_type = task_type
        # ä½¿ç”¨å…¨å±€å®¢æˆ·ç«¯ï¼Œé¿å…é‡å¤åˆ›å»º
        self.deepseek_client = get_deepseek_client()

    def execute(self, task):
        """ç»Ÿä¸€çš„ä»»åŠ¡æ‰§è¡Œå…¥å£"""
        try:
            # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©æ‰§è¡Œæ–¹æ³•
            if self.task_type == "ner":
                prompt = self._build_ner_prompt(task)
            elif self.task_type == "relation":
                prompt = self._build_relation_prompt(task)
            elif self.task_type == "event":
                prompt = self._build_event_prompt(task)
            elif self.task_type == "preprocess":
                return self._execute_preprocess(task)
            elif self.task_type == "graph":
                return self._execute_graph_build(task)
            elif self.task_type == "semantic":
                return self._execute_semantic_enhancement(task)
            elif self.task_type == "standardize":
                return self._execute_entity_standardization(task)
            elif self.task_type == "complete_relations":
                return self._execute_relation_completion(task)
            elif self.task_type == "extract_document":
                return self._execute_document_extraction(task)
            else:
                return {"result": [], "error": "Unknown task type"}

            # è°ƒç”¨ DeepSeek API
            response = self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": f"You are a {self.task_type} agent. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )
            content = response.choices[0].message.content.strip()

            # æ¸…ç†å“åº”
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content) if content else {"result": [], "error": "Empty response"}
            return result

        except json.JSONDecodeError as e:
            print(f"{self.task_type} JSONè§£æé”™è¯¯: {str(e)}")
            return {"result": [], "error": f"Failed to parse response: {str(e)}"}
        except Exception as e:
            print(f"{self.task_type} é”™è¯¯: {str(e)}")
            return {"result": [], "error": str(e)}

    def _build_ner_prompt(self, task):
        text = task.get('text', '')
        return f"""ä½ æ˜¯çŸ¥è¯†å›¾è°±æ„å»ºä¸“å®¶ã€‚ä»æ–‡æœ¬ä¸­ç²¾ç¡®æå–5ç±»å®ä½“:äººç‰©ã€æœºæ„ã€åœ°ç‚¹ã€é¡¹ç›®ã€æ¦‚å¿µã€‚

æ–‡æœ¬: ```{text}```

æå–è§„åˆ™:
1. **äººç‰©(PERSON)** - å¿…é¡»æ˜¯å…·ä½“çš„äººå
2. **æœºæ„(ORGANIZATION)** - ç»„ç»‡ã€å…¬å¸ã€æ”¿åºœéƒ¨é—¨
3. **åœ°ç‚¹(LOCATION)** - åœ°ç†ä½ç½®ã€è®¾æ–½
4. **é¡¹ç›®(PROJECT)** - è®¡åˆ’ã€é¡¹ç›®ã€è¡ŒåŠ¨
5. **æ¦‚å¿µ(CONCEPT)** - æŠ½è±¡æ¦‚å¿µã€ç†è®ºã€åè®®ã€ç‰©å“

å…³é”®è¦æ±‚:
- ä¿æŒåŸè²Œ:ä¸è¦æ”¹å˜å®ä½“åç§°,ä½¿ç”¨æ–‡ä¸­çš„å®Œæ•´å½¢å¼
- æå–æ‰€æœ‰:æ¯ä¸ªç¬¦åˆæ¡ä»¶çš„å®ä½“éƒ½è¦æå–
- é¿å…æ³›åŒ–:ç»ä¸ä½¿ç”¨"äººç‰©"ã€"ç»„ç»‡"ç­‰æ³›åŒ–è¯æ±‡

è¾“å‡ºæ ¼å¼(çº¯JSON,æ— å…¶ä»–å†…å®¹):
{{
  "entities": [
    {{"word": "æç»´Â·å…‹ç½—å®", "label": "PERSON"}},
    {{"word": "æ˜Ÿè€€è”é‚¦", "label": "ORGANIZATION"}}
  ]
}}"""

    def _build_relation_prompt(self, task):
        entities = task.get('entities', [])
        text = task.get('text', '')
        entities_str = ', '.join([e['word'] for e in entities])

        return f"""# çŸ¥è¯†å›¾è°±å…³ç³»æŠ½å–ä»»åŠ¡

    ## ç¬¬ä¸€æ­¥ï¼šç†è§£æ–‡æœ¬
    æ–‡æœ¬å†…å®¹ï¼š
    {text}

    ## ç¬¬äºŒæ­¥ï¼šè¯†åˆ«å®ä½“
    å¯ç”¨å®ä½“åˆ—è¡¨ï¼ˆä¸»è¯­å’Œå®¾è¯­å¿…é¡»ä»è¿™é‡Œé€‰æ‹©ï¼‰ï¼š
    {entities_str}

    ## ç¬¬ä¸‰æ­¥ï¼šæå–å®Œæ•´å…³ç³»ä¸‰å…ƒç»„

    è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š
    1. æ‰¾å‡ºæ–‡æœ¬ä¸­å“ªäº›å®ä½“ä¹‹é—´å­˜åœ¨å…³ç³»
    2. ç¡®å®šå®ƒä»¬ä¹‹é—´çš„åŠ¨ä½œæˆ–å…³ç³»ï¼ˆå¿…é¡»æ˜¯å®Œæ•´çš„åŠ¨è¯çŸ­è¯­ï¼Œä¾‹å¦‚"æå‡º"ã€"å‘å¸ƒ"ã€"å±äº"ï¼‰
    3. ç»„ç»‡æˆ ä¸»è¯­-è°“è¯­-å®¾è¯­ æ ¼å¼

    ## å…³é”®è¦æ±‚ï¼š
    âœ“ è°“è¯­å¿…é¡»æ˜¯å®Œæ•´çš„åŠ¨è¯æˆ–åŠ¨è¯çŸ­è¯­ï¼ˆ2-4ä¸ªå­—ï¼‰
    âœ“ ä»åŸæ–‡ä¸­æå–ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´
    âœ“ ç¤ºä¾‹ï¼š
      - "å›½åŠ¡é™¢æå‡ºåå››äº”è§„åˆ’" â†’ predicate: "æå‡º" âœ“
      - "å…¬å¸å‘å¸ƒæ–°äº§å“" â†’ predicate: "å‘å¸ƒ" âœ“  
      - "å‘˜å·¥å±äºæŸéƒ¨é—¨" â†’ predicate: "å±äº" âœ“

    âœ— é¿å…ä½¿ç”¨å•å­—åŠ¨è¯ï¼š
      - predicate: "æ" âœ—
      - predicate: "å‘" âœ—
      - predicate: "å±" âœ—

    ## è¾“å‡ºæ ¼å¼
    ç›´æ¥è¾“å‡ºJSONæ•°ç»„ï¼Œä¸è¦ä»»ä½•è§£é‡Šï¼š
    [
      {{"subject": "å®ä½“A", "predicate": "å®Œæ•´åŠ¨è¯çŸ­è¯­", "object": "å®ä½“B"}}
    ]"""

    def _build_event_prompt(self, task):
        text = task.get('text', '')
        return f"""ä½ æ˜¯äº‹ä»¶æŠ½å–ä¸“å®¶ã€‚ä»æ–‡æœ¬ä¸­è¯†åˆ«å…³é”®äº‹ä»¶ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å«ï¼šæ—¶é—´ã€åœ°ç‚¹ã€å‚ä¸è€…ã€åŠ¨ä½œã€‚

    æ–‡æœ¬: ```{text}```

    æŠ½å–è§„åˆ™:
    1. è¯†åˆ«æ˜ç¡®å‘ç”Ÿçš„äº‹ä»¶ï¼ˆç­¾çº¦ã€ä¼šè®®ã€äº¤æ˜“ã€åè®®ç­‰ï¼‰
    2. åŒ…å«æ—¶é—´ä¿¡æ¯çš„ä¼˜å…ˆ
    3. æ¯ä¸ªäº‹ä»¶ç”¨ä¸€å¥è¯æè¿°

    è¿”å›æ ¼å¼(çº¯JSON):
    {{
      "events": [
        "2020å¹´9æœˆ1æ—¥ï¼Œæµ™æ±Ÿç”³å‹å››è¾¾ä¸å˜‰å–„å¿ç¬¬ä¸‰å¹¼å„¿å›­ç­¾è®¢ç§ŸèµåˆåŒ",
        "åˆåŒçº¦å®šç§ŸèµæœŸé™ä¸º5å¹´",
        "ç§Ÿèµé¢ç§¯ä¸º3779.70å¹³æ–¹ç±³"
      ]
    }}"""

    def _execute_preprocess(self, task):
        try:
            text = task['text']
            cleaned_text = ' '.join(text.split())
            return {
                "cleaned_text": cleaned_text,
                "original_length": len(text),
                "cleaned_length": len(cleaned_text)
            }
        except Exception as e:
            return {"cleaned_text": task.get('text', ''), "error": str(e)}

    def _execute_document_extraction(self, task):
        """æå–æ–‡æ¡£æ–‡æœ¬"""
        try:
            file_content = task['file_content']
            file_name = task['file_name']
            extension = os.path.splitext(file_name)[1].lower()

            if extension == '.txt':
                for encoding in ['utf-8', 'gbk', 'utf-16']:
                    try:
                        text = file_content.decode(encoding)
                        break
                    except UnicodeDecodeError:
                        continue
                else:
                    raise ValueError("æ— æ³•è§£ç æ–‡æœ¬æ–‡ä»¶")

            elif extension == '.docx':
                doc = Document(BytesIO(file_content))
                text = '\n'.join([para.text for para in doc.paragraphs if para.text.strip()])

            elif extension == '.pdf':
                text = ''
                with pdfplumber.open(BytesIO(file_content)) as pdf:
                    for page in pdf.pages:
                        text += page.extract_text() or ''
                text = text.strip()

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extension}")

            return {"extracted_text": text}
        except Exception as e:
            return {"extracted_text": "", "error": str(e)}

    def _execute_graph_build(self, task):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        try:
            entities = task.get('entities', [])
            relations = task.get('relations', [])
            enhanced_entities = task.get('enhanced_entities', {})
            disambiguated_entities = task.get('disambiguated_entities', [])

            print(f"ğŸ”¨ å¼€å§‹æ„å»ºå›¾è°±:")
            print(f"  - è¾“å…¥å®ä½“: {len(entities)}")
            print(f"  - è¾“å…¥å…³ç³»: {len(relations)}")

            nodes = []
            edges = []

            driver = GraphDatabase.driver(
                os.getenv("NEO4J_URI", "bolt://localhost:7687"),
                auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
            )

            with driver.session() as session:
                # åˆ›å»ºå®ä½“èŠ‚ç‚¹
                for e in entities:
                    entity_name = e['word']
                    entity_label = e['label']
                    original_name = e.get('original', entity_name)

                    disambig = next((d for d in disambiguated_entities
                                     if d.get('original', d.get('standardized', '')) == entity_name), None)
                    qid = disambig['disambiguated'] if disambig else None

                    enhanced = enhanced_entities.get(original_name, {}) if isinstance(enhanced_entities, dict) else {}
                    description = enhanced.get('enhanced_description', '')
                    instance_of = enhanced.get('instance_of', '')

                    try:
                        session.run(
                            """
                            MERGE (n:Entity {name: $name})
                            ON CREATE SET n.qid = $qid, n.label = $entity_label, 
                                         n.description = $description, n.instance_of = $instance_of,
                                         n.original_name = $original_name, n.created_at = datetime()
                            ON MATCH SET n.qid = COALESCE(n.qid, $qid), 
                                        n.label = $entity_label,
                                        n.description = COALESCE(n.description, $description),
                                        n.updated_at = datetime()
                            """,
                            name=entity_name, qid=qid, entity_label=entity_label,
                            description=description, instance_of=instance_of,
                            original_name=original_name
                        )
                        nodes.append({"id": qid or entity_name, "label": entity_name, "type": entity_label})
                    except Exception as e:
                        print(f"âš ï¸ åˆ›å»ºèŠ‚ç‚¹å¤±è´¥ {entity_name}: {str(e)}")

                print(f"âœ“ åˆ›å»ºäº† {len(nodes)} ä¸ªèŠ‚ç‚¹")

                # åˆ›å»ºå…³ç³»
                # åˆ›å»ºå…³ç³»
                for rel in relations:
                    if len(rel) == 3:
                        s, r, t = rel

                        # éªŒè¯å…³ç³»ç±»å‹ä¸ä¸ºç©º
                        if not r or not r.strip():
                            print(f"âš ï¸ è·³è¿‡ç©ºå…³ç³»: {s} -> ??? -> {t}")
                            continue

                        # éªŒè¯æºå’Œç›®æ ‡å®ä½“æ˜¯å¦å­˜åœ¨
                        entity_names = [e['word'] for e in entities]
                        if s not in entity_names:
                            print(f"âš ï¸ æºå®ä½“ä¸å­˜åœ¨: {s}")
                            continue
                        if t not in entity_names:
                            print(f"âš ï¸ ç›®æ ‡å®ä½“ä¸å­˜åœ¨: {t}")
                            continue

                        is_inferred = task.get('completed_relations') and rel in task.get('completed_relations', [])

                        try:
                            result = session.run(
                                """
                                MATCH (a:Entity {name: $s})
                                MATCH (b:Entity {name: $t})
                                MERGE (a)-[rel:REL {type: $r}]->(b)
                                ON CREATE SET rel.inferred = $inferred, rel.created_at = datetime()
                                ON MATCH SET rel.updated_at = datetime()
                                RETURN count(rel) as cnt
                                """,
                                s=s, t=t, r=r, inferred=is_inferred
                            )

                            record = result.single()
                            if record and record['cnt'] > 0:
                                edges.append({"source": s, "target": t, "type": r, "inferred": is_inferred})
                            else:
                                print(f"âš ï¸ å…³ç³»åˆ›å»ºå¤±è´¥: {s} -> {r} -> {t}")

                        except Exception as e:
                            print(f"âš ï¸ åˆ›å»ºå…³ç³»å¼‚å¸¸ {s}-{r}->{t}: {str(e)}")

                print(f"âœ“ åˆ›å»ºäº† {len(edges)} æ¡å…³ç³»")

            driver.close()

            result_msg = f"æˆåŠŸæ„å»ºå›¾è°±: {len(nodes)} èŠ‚ç‚¹, {len(edges)} å…³ç³»"
            print(f"âœ… {result_msg}")

            return {
                "graph": {"nodes": nodes, "edges": edges},
                "message": result_msg
            }

        except Exception as e:
            error_msg = f"å›¾è°±æ„å»ºå¼‚å¸¸: {str(e)}"
            print(f"âŒ {error_msg}")
            import traceback
            traceback.print_exc()
            return {
                "graph": {"nodes": [], "edges": []},
                "message": error_msg,
                "error": str(e)
            }

    def _execute_semantic_enhancement(self, task):
        """è¯­ä¹‰å¢å¼º"""
        try:
            entities = task.get('entities', [])
            enhanced_entities = []
            knowledge = ExternalKnowledge()

            for e in entities:
                entity_name = e['word']
                wikidata_info = knowledge.query_wikidata_entity(entity_name, 'zh')
                if wikidata_info:
                    enhanced_entities.append({
                        'original': entity_name,
                        'qid': wikidata_info['qid'],
                        'enhanced_description': wikidata_info['description'],
                        'instance_of': wikidata_info['instance_of'],
                        'location': wikidata_info['location']
                    })
                else:
                    enhanced_entities.append({'original': entity_name, 'enhanced': False})

            return {
                "enhanced": len(enhanced_entities) > 0,
                "details": enhanced_entities,
                "message": f"å¢å¼ºäº† {len(enhanced_entities)} ä¸ªå®ä½“"
            }
        except Exception as e:
            return {"enhanced": False, "details": [], "error": str(e)}

    def _execute_entity_standardization(self, task):
        """å®ä½“æ ‡å‡†åŒ–"""
        try:
            entities = task.get('entities', [])
            if not entities:
                return {"standardized_mapping": {}, "standardized_entities": []}

            entity_list = [e['word'] for e in entities]
            prompt = f"""ä½ æ˜¯ä¸€ä½å®ä½“æ¶ˆæ­§ä¸çŸ¥è¯†è¡¨ç¤ºé¢†åŸŸçš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“åç§°è¿›è¡Œæ ‡å‡†åŒ–,ä»¥ç¡®ä¿å‘½åçš„ä¸€è‡´æ€§ã€‚

    ä»¥ä¸‹æ˜¯ä»çŸ¥è¯†å›¾è°±ä¸­æå–çš„ä¸€ç»„å®ä½“åç§°,å…¶ä¸­éƒ¨åˆ†å®ä½“å¯èƒ½æŒ‡ä»£åŒä¸€çœŸå®ä¸–ç•Œæ¦‚å¿µ,ä½†è¡¨è¿°æ–¹å¼ä¸åŒã€‚è¯·å¯¹è¿™äº›å®ä½“è¿›è¡Œå½’ä¸€åŒ–,è¯†åˆ«å‡ºæŒ‡å‘åŒä¸€æ¦‚å¿µçš„å®ä½“å˜ä½“,å¹¶ä¸ºæ¯ç»„å˜ä½“æŒ‡å®šä¸€ä¸ªç»Ÿä¸€çš„æ ‡å‡†åç§°ã€‚

    å®ä½“åˆ—è¡¨:{entity_list}

    è§„åˆ™:
    1. åªå½’å¹¶ç¡®å®šæŒ‡å‘åŒä¸€æ¦‚å¿µçš„å®ä½“
    2. æ ‡å‡†åç§°åº”è¯¥æ˜¯æœ€å®Œæ•´ã€æœ€è§„èŒƒçš„å½¢å¼
    3. è€ƒè™‘ç¼©å†™ã€ç®€ç§°ã€å…¨ç§°çš„å¯¹åº”å…³ç³»

    è¾“å‡ºæ ¼å¼ç¤ºä¾‹:
    {{
      "æ ‡å‡†åç§°1": ["å˜ä½“åç§°1", "å˜ä½“åç§°2"],
      "æ ‡å‡†åç§°2": ["å˜ä½“åç§°3", "å˜ä½“åç§°4"]
    }}

    å¦‚æœæ²¡æœ‰éœ€è¦æ ‡å‡†åŒ–çš„å®ä½“,è¿”å›ç©ºå¯¹è±¡: {{}}

    åªè¾“å‡ºJSON,ä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

            response = self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1
            )

            content = response.choices[0].message.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            standardized = json.loads(content) if content else {}

            # åˆ›å»ºæ˜ å°„:æ‰€æœ‰å˜ä½“ -> æ ‡å‡†å
            variant_to_standard = {}
            for standard_name, variants in standardized.items():
                for variant in variants:
                    variant_to_standard[variant] = standard_name
                variant_to_standard[standard_name] = standard_name

            # æ ‡å‡†åŒ–å®ä½“
            standardized_entities = []
            seen_standards = set()

            for entity in entities:
                original_word = entity['word']
                standard_name = variant_to_standard.get(original_word, original_word)

                if standard_name not in seen_standards:
                    standardized_entities.append({
                        'word': standard_name,
                        'label': entity['label'],
                        'original': original_word,
                        'variants': standardized.get(standard_name, [])
                    })
                    seen_standards.add(standard_name)

            validated_entities = self._validate_entities(standardized_entities)

            print(f"âœ“ æ ‡å‡†åŒ–: {len(entities)} -> {len(validated_entities)} ä¸ªå®ä½“")
            print(f"âœ“ åˆå¹¶æ˜ å°„: {standardized}")

            return {
                "standardized_mapping": standardized,
                "standardized_entities": validated_entities,
                "entity_mapping": variant_to_standard
            }

        except Exception as e:
            print(f"å®ä½“æ ‡å‡†åŒ–é”™è¯¯: {str(e)}")
            return {"standardized_mapping": {}, "standardized_entities": entities, "error": str(e)}

    def _validate_entities(self, entities):
        """éªŒè¯å’Œè¿‡æ»¤å®ä½“"""
        generic_terms = {'äººç‰©', 'ç»„ç»‡', 'æœºæ„', 'åœ°ç‚¹', 'å®¶æ—', 'å…¬å¸',
                         'æ”¿åºœ', 'é¡¹ç›®', 'è®¡åˆ’', 'æ¦‚å¿µ', 'ç†è®º', 'åè®®'}
        validated = []

        for entity in entities:
            word = entity.get('word', '')
            original = entity.get('original', word)

            # æ£€æŸ¥æ˜¯å¦è¿‡åº¦æ³›åŒ–
            if word in generic_terms:
                print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦æ³›åŒ–: {word} (åŸå: {original})")
                entity['word'] = original  # æ¢å¤åŸå

            # æ£€æŸ¥æ˜¯å¦è¿‡çŸ­
            if len(word.strip()) < 2:
                print(f"âš ï¸ è·³è¿‡è¿‡çŸ­å®ä½“: {word}")
                continue

            validated.append(entity)

        print(f"âœ“ éªŒè¯åä¿ç•™ {len(validated)} ä¸ªå®ä½“")
        return validated

    def _execute_relation_completion(self, task):
        """å…³ç³»è¡¥å…¨"""
        try:
            entities = task.get('entities', [])
            existing_relations = task.get('relations', [])

            if len(entities) < 2:
                return {"completed_relations": []}

            mid = len(entities) // 2
            community1 = [e['word'] for e in entities[:mid]][:5]
            community2 = [e['word'] for e in entities[mid:]][:5]

            if not community1 or not community2:
                return {"completed_relations": []}

            relations_text = '\n'.join([f"{r[0]} - {r[1]} - {r[2]}" for r in existing_relations[:10]])

            prompt = f"""ä½ æ˜¯ä¸€ä½çŸ¥è¯†è¡¨ç¤ºä¸æ¨ç†æ–¹é¢çš„ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨çŸ¥è¯†å›¾è°±ä¸­æ¨æ–­å‡ºå½¼æ­¤æœªç›´æ¥è¿æ¥çš„å®ä½“ä¹‹é—´çš„å¯èƒ½å…³ç³»ã€‚

    ç¤¾åŒº1å®ä½“åˆ—è¡¨:{community1}
    ç¤¾åŒº2å®ä½“åˆ—è¡¨:{community2}

    ä»¥ä¸‹æ˜¯éƒ¨åˆ†æ¶‰åŠè¿™äº›å®ä½“çš„å·²æœ‰ä¸‰å…ƒç»„å…³ç³»:
    {relations_text}

    è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯,æ¨ç†å‡º2-3æ¡åœ¨ç¤¾åŒº1ä¸ç¤¾åŒº2ä¹‹é—´å¯èƒ½å­˜åœ¨çš„å…³ç³»ã€‚

    è¦æ±‚:
    (1) ä»…åŒ…å«é«˜åº¦å¯ä¿¡ã€è¯­ä¹‰æ˜ç¡®çš„å…³ç³»
    (2) è°“è¯­å¿…é¡»ç®€æ´æ˜äº†,é™åˆ¶ä¸ºæœ€å¤š2ä¸ªè¯,æ¨èä½¿ç”¨1ä¸ªè¯
    (3) ç¦æ­¢ä¸»å®¾å®ä½“ç›¸åŒ,ä¸å¾—å‡ºç°è‡ªæŒ‡å…³ç³»
    (4) æ‰€é€‰è°“è¯­åº”èƒ½å‡†ç¡®æè¿°ä¸¤å®ä½“ä¹‹é—´çš„å…·ä½“è”ç³»

    è¾“å‡ºæ ¼å¼:
    [
      {{"subject": "ç¤¾åŒº1ä¸­çš„å®ä½“", "predicate": "æ¨ç†å…³ç³»", "object": "ç¤¾åŒº2ä¸­çš„å®ä½“"}}
    ]

    åªè¾“å‡ºJSONæ•°ç»„,ä¸è¦ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

            response = self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "Return only valid JSON array."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )

            content = response.choices[0].message.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            completed_relations = json.loads(content) if content else []

            formatted_relations = []
            for rel in completed_relations:
                if isinstance(rel, dict) and all(k in rel for k in ['subject', 'predicate', 'object']):
                    # ç¡®ä¿subjectå’Œobjectä¸ç›¸åŒ
                    if rel['subject'] != rel['object']:
                        formatted_relations.append([rel['subject'], rel['predicate'], rel['object']])

            return {"completed_relations": formatted_relations}

        except Exception as e:
            return {"completed_relations": [], "error": str(e)}


# ===========================================
# å…·ä½“æ™ºèƒ½ä½“å®ç°
# ===========================================

class PreprocessorAgent(CustomAgent):
    def __init__(self):
        super().__init__("Preprocessor", "preprocess")


class NERAgent(CustomAgent):
    def __init__(self):
        super().__init__("NER", "ner")


class RelationExtractionAgent(CustomAgent):
    def __init__(self):
        super().__init__("RelationExtraction", "relation")


class EventExtractionAgent(CustomAgent):
    def __init__(self):
        super().__init__("EventExtraction", "event")


class EntityStandardizationAgent(CustomAgent):
    def __init__(self):
        super().__init__("EntityStandardization", "standardize")


class RelationCompletionAgent(CustomAgent):
    def __init__(self):
        super().__init__("RelationCompletion", "complete_relations")


class GraphBuilderAgent(CustomAgent):
    def __init__(self):
        super().__init__("GraphBuilder", "graph")


class SemanticEnhancementAgent(CustomAgent):
    def __init__(self):
        super().__init__("SemanticEnhancement", "semantic")


class DocumentExtractorAgent(CustomAgent):
    def __init__(self):
        super().__init__("DocumentExtractor", "extract_document")


# ===========================================
# å¼‚æ­¥å¤„ç†å‡½æ•°
# ===========================================

async def async_process_document_task(file_content: bytes, file_name: str, task_id: str):
    """å¼‚æ­¥æ–‡æ¡£å¤„ç†åå°ä»»åŠ¡"""
    try:
        start_time = time.time()
        processing_status[task_id] = {
            "status": "processing",
            "progress": 5,
            "message": "å¼€å§‹å¤„ç†...",
            "created_at": time.time()
        }

        # 1. æ–‡æ¡£æå– - ç›´æ¥å®ä¾‹åŒ–
        processing_status[task_id].update({"progress": 10, "message": "æå–æ–‡æ¡£å†…å®¹..."})
        extractor = DocumentExtractorAgent()
        extract_task = {"type": "extract_document", "file_content": file_content, "file_name": file_name}
        extracted = extractor.execute(extract_task)
        text = extracted.get("extracted_text", "")

        if not text:
            raise Exception(extracted.get("error", "æ–‡æ¡£æå–å¤±è´¥"))

        # 2. é¢„å¤„ç†
        processing_status[task_id].update({"progress": 20, "message": "é¢„å¤„ç†æ–‡æœ¬..."})
        cleaned_text = ' '.join(text.split())

        # 3. å¹¶è¡Œæ‰§è¡Œ NER å’Œäº‹ä»¶æŠ½å–
        processing_status[task_id].update({"progress": 30, "message": "å¹¶è¡Œæå–å®ä½“å’Œäº‹ä»¶..."})
        entities_task = call_deepseek_api(
            f"""ä»æ–‡æœ¬ä¸­æå–å®ä½“ã€‚æ–‡æœ¬: ```{cleaned_text}```
è¿”å›JSON: {{"entities": [{{"word": "å®ä½“", "label": "ç±»å‹"}}]}}""", "ner"
        )
        events_task = call_deepseek_api(
            f"""ä»æ–‡æœ¬ä¸­æå–äº‹ä»¶ã€‚æ–‡æœ¬: ```{cleaned_text}```
è¿”å›JSON: {{"events": ["äº‹ä»¶1"]}}""", "event"
        )

        entities_result, events_result = await asyncio.gather(
            entities_task, events_task, return_exceptions=True
        )

        if isinstance(events_result, Exception):
            print(f"âš ï¸ äº‹ä»¶æŠ½å–å¤±è´¥: {str(events_result)}")
            events_result = {"events": []}
        else:
            print(f"âœ“ äº‹ä»¶æŠ½å–æˆåŠŸ: {events_result}")

        print(f"âœ“ å®ä½“æ•°é‡: {len(entities_result.get('entities', []))}")
        print(f"âœ“ äº‹ä»¶æ•°é‡: {len(events_result.get('events', []))}")
        print(f"âœ“ äº‹ä»¶å†…å®¹: {events_result.get('events', [])[:3]}")  # æ˜¾ç¤ºå‰3ä¸ªäº‹ä»¶_execute_graph_build

        if isinstance(entities_result, Exception):
            entities_result = {"entities": []}
        if isinstance(events_result, Exception):
            events_result = {"events": []}

        # 4. å®ä½“æ ‡å‡†åŒ–ï¼ˆæå‰åˆ°å…³ç³»æŠ½å–ä¹‹å‰ï¼‰
        processing_status[task_id].update({"progress": 50, "message": "å®ä½“æ ‡å‡†åŒ–..."})
        standardization_agent = EntityStandardizationAgent()
        standardize_task = {"type": "standardize", "entities": entities_result.get("entities", [])}
        standardized = standardization_agent.execute(standardize_task)
        standardized_entities = standardized.get("standardized_entities", entities_result.get("entities", []))

        # 5. å…³ç³»æŠ½å–ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–åçš„å®ä½“ï¼‰
        processing_status[task_id].update({"progress": 60, "message": "æå–å…³ç³»..."})
        if standardized_entities:
            entities_str = ', '.join([e['word'] for e in standardized_entities])
            relations_result = await call_deepseek_api(
                f"""ä»æ–‡æœ¬ä¸­æå–å®ä½“é—´çš„å…³ç³»ã€‚

        æ–‡æœ¬: ```{cleaned_text}```

        **å¯ç”¨å®ä½“ï¼ˆå¿…é¡»ç²¾ç¡®ä½¿ç”¨ï¼‰**: 
        {entities_str}

        **è§„åˆ™**:
        1. ä¸»è¯­å’Œå®¾è¯­å¿…é¡»ä»ä¸Šè¿°å®ä½“åˆ—è¡¨ä¸­ç²¾ç¡®é€‰æ‹©
        2. è°“è¯­ä½¿ç”¨ç®€çŸ­åŠ¨è¯ï¼ˆ1-2å­—ï¼‰
        3. åªæå–æ–‡æœ¬ä¸­æ˜ç¡®çš„å…³ç³»

        è¿”å›JSON: {{"relations": [["å®ä½“1", "åŠ¨è¯", "å®ä½“2"]]}}""", "relation"
            )
        else:
            relations_result = {"relations": []}

        print(f"âœ“ å…³ç³»æ•°é‡: {len(relations_result.get('relations', []))}")

        # 6. å…³ç³»è¡¥å…¨ - ç›´æ¥å®ä¾‹åŒ–
        processing_status[task_id].update({"progress": 70, "message": "å…³ç³»è¡¥å…¨..."})
        completion_agent = RelationCompletionAgent()
        complete_task = {
            "type": "complete_relations",
            "entities": standardized_entities,
            "relations": relations_result.get("relations", [])
        }
        completed = completion_agent.execute(complete_task)
        all_relations = relations_result.get("relations", []) + completed.get("completed_relations", [])

        # 7. è¯­ä¹‰å¢å¼º - ç›´æ¥å®ä¾‹åŒ–
        processing_status[task_id].update({"progress": 80, "message": "è¯­ä¹‰å¢å¼º..."})
        semantic_agent = SemanticEnhancementAgent()
        semantic_task = {"type": "semantic", "entities": standardized_entities}
        enhanced = semantic_agent.execute(semantic_task)

        # 8. å®ä½“æ¶ˆæ­§
        disambiguated = []
        for e in standardized_entities:
            disambig_entity = disambiguator.disambiguate(e.get('original', e['word']), text)
            disambiguated.append({
                "original": e.get('original', e['word']),
                "standardized": e['word'],
                "disambiguated": disambig_entity,
                "label": e['label']
            })

        # 8.5 å…³ç³»å®ä½“åç§°ä¿®æ­£
        processing_status[task_id].update({"progress": 85, "message": "ä¿®æ­£å…³ç³»..."})

        entity_names = {e['word'] for e in standardized_entities}
        entity_name_list = [e['word'] for e in standardized_entities]

        corrected_relations = []
        skipped_relations = []

        for rel in all_relations:
            if len(rel) != 3:
                continue

            s, r, t = rel
            original_s, original_t = s, t
            s_found, t_found = False, False

            # ç²¾ç¡®åŒ¹é…æ£€æŸ¥
            if s in entity_names:
                s_found = True
            if t in entity_names:
                t_found = True

            # å¦‚æœæºå®ä½“æœªæ‰¾åˆ°,å°è¯•æ¨¡ç³ŠåŒ¹é…
            if not s_found:
                for entity in entity_name_list:
                    # æ–¹æ³•1: åŒ…å«å…³ç³»
                    if s in entity or entity in s:
                        s = entity
                        s_found = True
                        print(f"âœ“ ä¿®æ­£æºå®ä½“: '{original_s}' -> '{entity}'")
                        break
                    # æ–¹æ³•2: å»é™¤"åŸºäº"ç­‰å‰ç¼€ååŒ¹é…
                    s_clean = s.replace('åŸºäº', '').replace('çš„', '').replace('ä¸', '').replace('åä½œ', '').strip()
                    if len(s_clean) >= 3 and s_clean in entity:
                        s = entity
                        s_found = True
                        print(f"âœ“ ä¿®æ­£æºå®ä½“: '{original_s}' -> '{entity}'")
                        break

            # å¦‚æœç›®æ ‡å®ä½“æœªæ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            if not t_found:
                for entity in entity_name_list:
                    if t in entity or entity in t:
                        t = entity
                        t_found = True
                        print(f"âœ“ ä¿®æ­£ç›®æ ‡å®ä½“: '{original_t}' -> '{entity}'")
                        break
                    t_clean = t.replace('åŸºäº', '').replace('çš„', '').replace('ä¸', '')
                    e_clean = entity.replace('åŸºäº', '').replace('çš„', '').replace('ä¸', '')
                    if t_clean and e_clean and (t_clean in e_clean or e_clean in t_clean):
                        t = entity
                        t_found = True
                        print(f"âœ“ ä¿®æ­£ç›®æ ‡å®ä½“: '{original_t}' -> '{entity}'")
                        break

            # åªä¿ç•™ä¸¤ç«¯å®ä½“éƒ½æ‰¾åˆ°çš„å…³ç³»
            if s_found and t_found:
                corrected_relations.append([s, r, t])
            else:
                skipped_relations.append(rel)
                if not s_found:
                    print(f"âš ï¸ æ— æ³•ä¿®æ­£æºå®ä½“: {original_s}")
                if not t_found:
                    print(f"âš ï¸ æ— æ³•ä¿®æ­£ç›®æ ‡å®ä½“: {original_t}")

        print(f"âœ… å…³ç³»ä¿®æ­£å®Œæˆ: {len(all_relations)} -> {len(corrected_relations)} (è·³è¿‡ {len(skipped_relations)})")
        all_relations = corrected_relations

        # 9. å›¾æ„å»º - ç›´æ¥å®ä¾‹åŒ–
        processing_status[task_id].update({"progress": 90, "message": "æ„å»ºçŸ¥è¯†å›¾è°±..."})
        enhanced_details = enhanced.get("details", [])
        enhanced_map = {item['original']: item for item in enhanced_details}

        print(f"ğŸ“Š å‡†å¤‡æ„å»ºå›¾è°±:")
        print(f"  - å®ä½“æ•°: {len(standardized_entities)}")
        print(f"  - å…³ç³»æ•°: {len(all_relations)}")
        print(f"  - å¢å¼ºå®ä½“æ•°: {len(enhanced_map)}")

        graph_agent = GraphBuilderAgent()
        graph_task = {
            "type": "graph",
            "entities": standardized_entities,
            "relations": all_relations,
            "enhanced_entities": enhanced_map,
            "disambiguated_entities": disambiguated,
            "completed_relations": completed.get("completed_relations", [])
        }

        try:
            graph = graph_agent.execute(graph_task)
            print(f"âœ… å›¾è°±æ„å»ºç»“æœ: {graph.get('message', 'Unknown')}")

            if 'error' in graph:
                print(f"âš ï¸ å›¾è°±æ„å»ºè­¦å‘Š: {graph.get('error')}")

        except Exception as e:
            print(f"âŒ å›¾è°±æ„å»ºå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            graph = {
                "graph": {"nodes": [], "edges": []},
                "message": f"å›¾è°±æ„å»ºå¤±è´¥: {str(e)}"
            }
        graph = graph_agent.execute(graph_task)

        processing_time = time.time() - start_time

        # æ„å»ºå®Œæ•´ç»“æœ
        result_data = {
            "status": "success",
            "preprocessed": {
                "cleaned_text": cleaned_text[:1000] + "..." if len(cleaned_text) > 1000 else cleaned_text,
                "original_length": len(text),
                "cleaned_length": len(cleaned_text)
            },
            "entities": entities_result,
            "entity_standardization": standardized,
            "relations": relations_result,
            "relation_completion": completed,
            "all_relations": {"relations": all_relations},
            "events": events_result,
            "semantic_enhancement": enhanced,
            "graph": graph,
            "disambiguated_entities": disambiguated
        }

        processing_status[task_id] = {
            "status": "completed",
            "progress": 100,
            "message": "å¤„ç†å®Œæˆ",
            "processing_time": processing_time,
            "created_at": processing_status[task_id]["created_at"],
            "result": result_data
        }

        # æ·»åŠ é¢å¤–æ—¥å¿—
        print(f"âœ… ä»»åŠ¡å®Œæˆ:")
        print(f"  - ä»»åŠ¡ID: {task_id}")
        print(f"  - èŠ‚ç‚¹æ•°: {len(graph.get('graph', {}).get('nodes', []))}")
        print(f"  - è¾¹æ•°: {len(graph.get('graph', {}).get('edges', []))}")
        print(f"  - äº‹ä»¶æ•°: {len(events_result.get('events', []))}")

        save_analysis_result(task_id, file_name, result_data, processing_time)

    except Exception as e:
        print(f"âŒ å¼‚æ­¥å¤„ç†å¤±è´¥: {str(e)}")
        processing_status[task_id] = {
            "status": "error",
            "progress": 0,
            "message": f"å¤„ç†å¤±è´¥: {str(e)}",
            "error": str(e),
            "created_at": processing_status.get(task_id, {}).get("created_at", time.time())
        }


# æ›´å¥½çš„ç¼–ç æ£€æµ‹
def detect_encoding(content):
    """æ£€æµ‹æ–‡ä»¶ç¼–ç """
    encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'big5']

    for encoding in encodings:
        try:
            content.decode(encoding)
            return encoding
        except (UnicodeDecodeError, AttributeError):
            continue

    return 'utf-8'  # é»˜è®¤è¿”å› UTF-8

# ===========================================
# åˆå§‹åŒ–å…¨å±€å¯¹è±¡
# ===========================================

scheduler = MultiAgentScheduler()
disambiguator = EntityDisambiguation()


# ===========================================
# FastAPIåº”ç”¨é…ç½®
# ===========================================

async def cleanup_old_tasks():
    """æ¸…ç†24å°æ—¶å‰çš„ä»»åŠ¡çŠ¶æ€"""
    while True:
        try:
            current_time = time.time()
            tasks_to_remove = [
                task_id for task_id, status in processing_status.items()
                if current_time - status.get("created_at", current_time) > 24 * 3600
            ]
            for task_id in tasks_to_remove:
                del processing_status[task_id]
            if tasks_to_remove:
                print(f"ğŸ§¹ æ¸…ç†äº† {len(tasks_to_remove)} ä¸ªæ—§ä»»åŠ¡")
        except Exception as e:
            print(f"æ¸…ç†ä»»åŠ¡æ—¶å‡ºé”™: {e}")
        await asyncio.sleep(3600)


@asynccontextmanager
async def lifespan(app):
    print("ğŸš€ å¯åŠ¨çŸ¥è¯†å›¾è°±ç³»ç»Ÿ...")

    # æ•°æ®åº“ä¼˜åŒ–
    try:
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )
        with driver.session() as session:
            indexes = [
                "CREATE INDEX entity_name IF NOT EXISTS FOR (n:Entity) ON (n.name)",
                "CREATE INDEX entity_qid IF NOT EXISTS FOR (n:Entity) ON (n.qid)",
                "CREATE INDEX entity_label IF NOT EXISTS FOR (n:Entity) ON (n.label)",
            ]
            for index_query in indexes:
                try:
                    session.run(index_query)
                except Exception as e:
                    print(f"ç´¢å¼•åˆ›å»ºè­¦å‘Š: {e}")
        driver.close()
        print("âœ… æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")

    # æ³¨å†Œæ‰€æœ‰æ™ºèƒ½ä½“ï¼ˆç¡®ä¿æŒ‰æ­£ç¡®é¡ºåºï¼‰
    agents = [
        DocumentExtractorAgent(),  # æ–‡æ¡£æå–ï¼ˆç¬¬ä¸€ä¸ªï¼‰
        PreprocessorAgent(),  # é¢„å¤„ç†
        NERAgent(),  # å®ä½“è¯†åˆ«
        RelationExtractionAgent(),  # å…³ç³»æŠ½å–
        EventExtractionAgent(),  # äº‹ä»¶æŠ½å–
        EntityStandardizationAgent(),  # å®ä½“æ ‡å‡†åŒ–
        RelationCompletionAgent(),  # å…³ç³»è¡¥å…¨
        SemanticEnhancementAgent(),  # è¯­ä¹‰å¢å¼º
        GraphBuilderAgent(),  # å›¾æ„å»º
    ]

    for agent in agents:
        scheduler.register_agent(agent)
        print(f"  âœ“ æ³¨å†Œ {agent.name}")

    print("âœ… æ‰€æœ‰æ™ºèƒ½ä½“æ³¨å†ŒæˆåŠŸ")

    # å¯åŠ¨æ¸…ç†ä»»åŠ¡
    cleanup_task = asyncio.create_task(cleanup_old_tasks())
    print("âœ… åå°æ¸…ç†ä»»åŠ¡å¯åŠ¨")

    yield

    cleanup_task.cancel()
    print("ğŸ›‘ åº”ç”¨å…³é—­")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="çŸ¥è¯†å›¾è°±ç®¡ç†ç³»ç»Ÿ",
    description="æ™ºèƒ½æ–‡æ¡£åˆ†æä¸çŸ¥è¯†å›¾è°±æ„å»ºå¹³å°",
    version="2.0.0",
    lifespan=lifespan
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
@app.middleware("http")
async def performance_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time

    if process_time > 2.0:
        print(f"âš ï¸ æ…¢è¯·æ±‚: {request.url.path} è€—æ—¶ {process_time:.2f}s")

    response.headers["X-Process-Time"] = str(process_time)
    return response


# ===========================================
# APIè·¯ç”±
# ===========================================

@app.post("/process_document_async")
async def process_document_async(
        file: UploadFile = File(...),
        background_tasks: BackgroundTasks = BackgroundTasks()
):
    """å¼‚æ­¥æ–‡æ¡£å¤„ç†ç«¯ç‚¹"""
    try:
        task_id = str(uuid.uuid4())
        file_content = await file.read()

        if not file.filename:
            raise HTTPException(status_code=400, detail="æ–‡ä»¶åä¸èƒ½ä¸ºç©º")

        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in ['.txt', '.docx', '.pdf']:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")

        background_tasks.add_task(async_process_document_task, file_content, file.filename, task_id)

        return {"task_id": task_id, "status": "started", "message": "æ–‡æ¡£å¤„ç†å·²å¼€å§‹"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/processing_status/{task_id}")
async def get_processing_status(task_id: str):
    """è·å–å¤„ç†çŠ¶æ€"""
    status = processing_status.get(task_id)
    if not status:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    return status


@app.get("/get_graph")
async def get_graph():
    """è·å–å›¾è°±æ•°æ®"""
    try:
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )

        with driver.session() as session:
            # è·å–æ‰€æœ‰èŠ‚ç‚¹(åŒ…æ‹¬å­¤ç«‹èŠ‚ç‚¹)
            nodes_result = session.run("""
                MATCH (n:Entity)
                RETURN n.name as name, n.label as label, n.qid as qid,
                       n.description as description, n.instance_of as instance_of
                LIMIT 1000
            """)

            nodes_data = []
            node_types = {}
            for record in nodes_result:
                name = record["name"]
                label = record["label"]
                nodes_data.append({
                    "id": name,
                    "label": name,
                    "type": label or "Unknown",
                    "qid": record["qid"],
                    "description": record["description"]
                })
                node_types[name] = label or "Unknown"

            # è·å–æ‰€æœ‰å…³ç³»
            edges_result = session.run("""
                MATCH (n:Entity)-[r:REL]->(m:Entity)
                RETURN n.name as source, r.type as relation, m.name as target,
                       coalesce(r.inferred, false) as inferred
                LIMIT 1000
            """)

            edges = []
            for record in edges_result:
                edges.append({
                    "source": record["source"],
                    "target": record["target"],
                    "type": record["relation"],
                    "inferred": record["inferred"]
                })

            print(f"ğŸ“Š /get_graph è¿”å›: {len(nodes_data)} èŠ‚ç‚¹, {len(edges)} è¾¹")
            return {"nodes": nodes_data, "edges": edges}

        driver.close()
    except Exception as e:
        print(f"âŒ è·å–å›¾è°±å¤±è´¥: {str(e)}")
        return {"nodes": [], "edges": [], "error": str(e)}


@app.post("/clear_graph")
async def clear_graph():
    """æ¸…ç©ºå›¾è°±å’Œæ‰€æœ‰åˆ†æç»“æœ"""
    try:
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://neo4j:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )
        with driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")
        driver.close()

        if RESULTS_DIR.exists():
            for file in RESULTS_DIR.glob("*.json"):
                file.unlink()

        if cache_manager:
            try:
                cache_manager.flushdb()
            except Exception as e:
                print(f"âš ï¸ Redisæ¸…ç©ºå¤±è´¥: {e}")

        return {"status": "success", "message": "å›¾è°±å’Œæ‰€æœ‰åˆ†æç»“æœå·²æ¸…ç©º"}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/export_graph")
async def export_graph():
    """å¯¼å‡ºå›¾è°±å’Œåˆ†æç»“æœ"""
    try:
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://neo4j:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )

        with driver.session() as session:
            nodes_result = session.run("""
                MATCH (n:Entity)
                RETURN n.name as name, n.qid as qid, n.label as label,
                       n.description as description, n.instance_of as instance_of,
                       n.original_name as original_name
            """)
            nodes = [dict(record) for record in nodes_result]

            edges_result = session.run("""
                MATCH (a:Entity)-[r:REL]->(b:Entity)
                RETURN a.name as source, b.name as target, 
                       r.type as relation_type, r.inferred as inferred
            """)
            edges = [dict(record) for record in edges_result]

        driver.close()

        latest_analysis = None
        if RESULTS_DIR.exists():
            files = sorted(RESULTS_DIR.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
            if files:
                with open(files[0], 'r', encoding='utf-8') as f:
                    latest_analysis = json.load(f)

        export_data = {
            "version": "2.0",
            "export_time": datetime.now().isoformat(),
            "graph": {"nodes": nodes, "edges": edges},
            "statistics": {"node_count": len(nodes), "edge_count": len(edges)},
            "latest_analysis": latest_analysis
        }

        return JSONResponse(
            content=export_data,
            headers={"Content-Disposition": f"attachment; filename=kg_export_{int(time.time())}.json"}
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/import_graph")
async def import_graph(file: UploadFile = File(...)):
    """å¯¼å…¥å›¾è°±å’Œåˆ†æç»“æœ"""
    try:
        content = await file.read()
        graph_data = json.loads(content.decode('utf-8'))

        # å…¼å®¹ä¸åŒçš„JSONæ ¼å¼
        if "graph" in graph_data:
            # æ ¼å¼1: {"graph": {"nodes": [...], "edges": [...]}}
            nodes = graph_data["graph"].get("nodes", [])
            edges = graph_data["graph"].get("edges", [])
        elif "nodes" in graph_data and "edges" in graph_data:
            # æ ¼å¼2: {"nodes": [...], "edges": [...]}
            nodes = graph_data["nodes"]
            edges = graph_data["edges"]
        else:
            raise ValueError("æ— æ•ˆçš„JSONæ ¼å¼,ç¼ºå°‘nodesæˆ–edgeså­—æ®µ")

        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://neo4j:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )

        nodes_imported = 0
        edges_imported = 0

        with driver.session() as session:
            # å¯¼å…¥èŠ‚ç‚¹
            for node in nodes:
                try:
                    # å…¼å®¹ä¸åŒçš„èŠ‚ç‚¹å­—æ®µå
                    node_name = node.get('name') or node.get('id') or node.get('label')
                    if not node_name:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆèŠ‚ç‚¹: {node}")
                        continue

                    session.run("""
                        MERGE (n:Entity {name: $name})
                        SET n.qid = $qid, 
                            n.label = $label,
                            n.description = $description, 
                            n.instance_of = $instance_of,
                            n.original_name = $original_name, 
                            n.imported_at = datetime()
                    """,
                                name=node_name,
                                qid=node.get('qid'),
                                label=node.get('label') or node.get('type'),
                                description=node.get('description', ''),
                                instance_of=node.get('instance_of', ''),
                                original_name=node.get('original_name', node_name)
                                )
                    nodes_imported += 1
                except Exception as e:
                    print(f"âš ï¸ å¯¼å…¥èŠ‚ç‚¹å¤±è´¥ {node.get('name')}: {str(e)}")

            # å¯¼å…¥è¾¹
            for edge in edges:
                try:
                    # å…¼å®¹ä¸åŒçš„è¾¹å­—æ®µå
                    source = edge.get("source") or edge.get("from")
                    target = edge.get("target") or edge.get("to")
                    rel_type = edge.get("type") or edge.get("relation") or edge.get("relation_type")

                    if not source or not target or not rel_type:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆè¾¹: {edge}")
                        continue

                    session.run("""
                        MATCH (a:Entity {name: $source})
                        MATCH (b:Entity {name: $target})
                        MERGE (a)-[r:REL {type: $relation_type}]->(b)
                        SET r.inferred = $inferred, r.imported_at = datetime()
                    """,
                                source=source,
                                target=target,
                                relation_type=rel_type,
                                inferred=edge.get("inferred", False)
                                )
                    edges_imported += 1
                except Exception as e:
                    print(f"âš ï¸ å¯¼å…¥è¾¹å¤±è´¥ {edge.get('source')}->{edge.get('target')}: {str(e)}")

        driver.close()

        return {
            "status": "success",
            "message": f"æˆåŠŸå¯¼å…¥ {nodes_imported} ä¸ªèŠ‚ç‚¹å’Œ {edges_imported} æ¡å…³ç³»",
            "nodes_imported": nodes_imported,
            "edges_imported": edges_imported
        }

    except Exception as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {str(e)}")


@app.get("/get_latest_analysis")
async def get_latest_analysis():
    """è·å–æœ€æ–°çš„åˆ†æç»“æœ"""
    try:
        if not RESULTS_DIR.exists():
            return {"status": "no_results", "message": "æš‚æ— åˆ†æç»“æœ"}

        files = sorted(RESULTS_DIR.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)
        if not files:
            return {"status": "no_results", "message": "æš‚æ— åˆ†æç»“æœ"}

        with open(files[0], 'r', encoding='utf-8') as f:
            data = json.load(f)

        return {"status": "success", "data": data}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/get_analysis_history")
async def get_analysis_history(limit: int = 10):
    """è·å–å†å²åˆ†æè®°å½•åˆ—è¡¨"""
    try:
        if not RESULTS_DIR.exists():
            return {"status": "no_results", "history": []}

        files = sorted(RESULTS_DIR.glob("*.json"), key=lambda x: x.stat().st_mtime, reverse=True)

        history = []
        for file_path in files[:limit]:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    history.append({
                        "task_id": data.get("task_id"),
                        "file_name": data.get("file_name"),
                        "processed_at": data.get("processed_at"),
                        "processing_time": data.get("processing_time")
                    })
            except:
                continue

        return {"status": "success", "history": history}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/get_analysis_by_id/{task_id}")
async def get_analysis_by_id(task_id: str):
    """æ ¹æ®ä»»åŠ¡IDè·å–åˆ†æç»“æœ"""
    try:
        result_file = RESULTS_DIR / f"{task_id}.json"
        if not result_file.exists():
            raise HTTPException(status_code=404, detail="åˆ†æç»“æœä¸å­˜åœ¨")

        with open(result_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        return {"status": "success", "data": data}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    redis_status = "connected" if cache_manager else "disconnected"

    try:
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://localhost:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )
        with driver.session() as session:
            session.run("RETURN 1")
        neo4j_status = "connected"
        driver.close()
    except:
        neo4j_status = "error"

    return {
        "status": "healthy",
        "services": {"redis": redis_status, "neo4j": neo4j_status},
        "timestamp": time.time()
    }


@app.post("/import_analysis_csv")
async def import_analysis_csv(
        nodes_file: UploadFile = File(...),
        edges_file: UploadFile = File(None)
):
    """å¯¼å…¥CSVæ ¼å¼çš„åˆ†æç»“æœ"""
    try:
        # è¯»å–èŠ‚ç‚¹CSV
        nodes_content = await nodes_file.read()

        detected_encoding = detect_encoding(nodes_content)
        nodes_text = nodes_content.decode(detected_encoding)


        if not nodes_text:
            raise HTTPException(status_code=400, detail="æ— æ³•è§£ç èŠ‚ç‚¹CSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶ç¼–ç ")

        # è§£æèŠ‚ç‚¹CSV
        nodes_lines = nodes_text.strip().split('\n')
        if len(nodes_lines) < 2:
            raise HTTPException(status_code=400, detail="èŠ‚ç‚¹CSVæ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º")

        # ç®€å•CSVè§£æå‡½æ•°
        def parse_csv_line(line):
            parts = []
            current = ""
            in_quotes = False
            for char in line:
                if char == '"':
                    in_quotes = not in_quotes
                elif char == ',' and not in_quotes:
                    parts.append(current.strip().strip('"'))
                    current = ""
                else:
                    current += char
            parts.append(current.strip().strip('"'))
            return parts

        # è·³è¿‡è¡¨å¤´ï¼Œè§£æèŠ‚ç‚¹
        nodes = []
        for i, line in enumerate(nodes_lines[1:], start=2):
            if not line.strip():
                continue

            try:
                parts = parse_csv_line(line)

                if len(parts) < 2:
                    print(f"âš ï¸ è·³è¿‡ç¬¬{i}è¡Œï¼šå­—æ®µä¸è¶³")
                    continue

                node_name = parts[0].strip()
                node_label = parts[1].strip() if len(parts) > 1 else ""
                node_type = parts[2].strip() if len(parts) > 2 else "Unknown"
                node_description = parts[3].strip() if len(parts) > 3 else ""

                # éªŒè¯èŠ‚ç‚¹åç§°
                if not node_name:
                    print(f"âš ï¸ è·³è¿‡ç¬¬{i}è¡Œï¼šèŠ‚ç‚¹åç§°ä¸ºç©º")
                    continue

                nodes.append({
                    "name": node_name,
                    "label": node_label or node_name,
                    "type": node_type,
                    "description": node_description
                })
            except Exception as e:
                print(f"âš ï¸ è§£æç¬¬{i}è¡Œå¤±è´¥: {str(e)}")
                continue

        if not nodes:
            raise HTTPException(status_code=400, detail="æœªèƒ½è§£æå‡ºä»»ä½•æœ‰æ•ˆèŠ‚ç‚¹")

        # è§£æè¾¹CSVï¼ˆå¦‚æœæä¾›ï¼‰
        edges = []
        if edges_file:
            edges_content = await edges_file.read()

            # å°è¯•å¤šç§ç¼–ç 
            edges_text = None
            for encoding in ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']:
                try:
                    edges_text = edges_content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue

            if not edges_text:
                print("âš ï¸ æ— æ³•è§£ç è¾¹CSVæ–‡ä»¶ï¼Œè·³è¿‡è¾¹çš„å¯¼å…¥")
            else:
                edges_lines = edges_text.strip().split('\n')

                for i, line in enumerate(edges_lines[1:], start=2):
                    if not line.strip():
                        continue

                    try:
                        parts = parse_csv_line(line)

                        if len(parts) < 3:
                            print(f"âš ï¸ è·³è¿‡è¾¹ç¬¬{i}è¡Œï¼šå­—æ®µä¸è¶³")
                            continue

                        source = parts[0].strip()
                        rel_type = parts[1].strip()
                        target = parts[2].strip()
                        inferred = parts[3].strip().lower() == 'æ˜¯' if len(parts) > 3 else False

                        # éªŒè¯å¿…éœ€å­—æ®µ
                        if not source or not target or not rel_type:
                            print(f"âš ï¸ è·³è¿‡è¾¹ç¬¬{i}è¡Œï¼šæºèŠ‚ç‚¹ã€ç›®æ ‡èŠ‚ç‚¹æˆ–å…³ç³»ç±»å‹ä¸ºç©º")
                            continue

                        edges.append({
                            "source": source,
                            "type": rel_type,
                            "target": target,
                            "inferred": inferred
                        })
                    except Exception as e:
                        print(f"âš ï¸ è§£æè¾¹ç¬¬{i}è¡Œå¤±è´¥: {str(e)}")
                        continue

        # å¯¼å…¥åˆ°Neo4j
        driver = GraphDatabase.driver(
            os.getenv("NEO4J_URI", "bolt://neo4j:7687"),
            auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "12345678"))
        )

        nodes_imported = 0
        edges_imported = 0

        with driver.session() as session:
            # åˆ›å»ºèŠ‚ç‚¹
            for node in nodes:
                try:
                    session.run("""
                        MERGE (n:Entity {name: $name})
                        SET n.label = $label, n.type = $type,
                            n.description = $description, n.imported_at = datetime()
                    """, name=node["name"], label=node["label"],
                                type=node["type"], description=node["description"])
                    nodes_imported += 1
                except Exception as e:
                    print(f"âš ï¸ å¯¼å…¥èŠ‚ç‚¹å¤±è´¥ {node['name']}: {str(e)}")

            # åˆ›å»ºè¾¹
            for edge in edges:
                try:
                    session.run("""
                        MATCH (a:Entity {name: $source})
                        MATCH (b:Entity {name: $target})
                        MERGE (a)-[r:REL {type: $rel_type}]->(b)
                        SET r.inferred = $inferred, r.imported_at = datetime()
                    """, source=edge["source"], target=edge["target"],
                                rel_type=edge["type"], inferred=edge["inferred"])
                    edges_imported += 1
                except Exception as e:
                    print(f"âš ï¸ å¯¼å…¥è¾¹å¤±è´¥ {edge['source']}->{edge['target']}: {str(e)}")

        driver.close()

        return {
            "status": "success",
            "message": f"æˆåŠŸå¯¼å…¥ {nodes_imported} ä¸ªèŠ‚ç‚¹å’Œ {edges_imported} æ¡å…³ç³»",
            "nodes_imported": nodes_imported,
            "edges_imported": edges_imported,
            "nodes_total": len(nodes),
            "edges_total": len(edges)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ CSVå¯¼å…¥é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"å¯¼å…¥å¤±è´¥: {str(e)}")


# æŒ‚è½½é™æ€æ–‡ä»¶
current_dir = os.path.dirname(os.path.abspath(__file__))
static_dir = os.path.join(current_dir, "../static")
if os.path.exists(static_dir):
    app.mount("/", StaticFiles(directory=static_dir, html=True), name="static")

# ===========================================
# ä¸»å‡½æ•°
# ===========================================

if __name__ == "__main__":
    print("ğŸŒ å¯åŠ¨çŸ¥è¯†å›¾è°±ç®¡ç†ç³»ç»Ÿ...")
    print("ğŸ“Š åŠŸèƒ½ç‰¹æ€§:")
    print("   - å¼‚æ­¥æ–‡æ¡£å¤„ç†")
    print("   - Redisç¼“å­˜åŠ é€Ÿ")
    print("   - å®ä½“æ ‡å‡†åŒ–ä¸éªŒè¯")
    print("   - å…³ç³»æ¨ç†è¡¥å…¨")
    print()
    print("ğŸ”— è®¿é—®åœ°å€:")
    print("   - å‰ç«¯ç•Œé¢: http://localhost:8000")
    print("   - APIæ–‡æ¡£: http://localhost:8000/docs")
    print("   - å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print()

    uvicorn.run(app, host="0.0.0.0", port=8000, workers=1, log_level="info")